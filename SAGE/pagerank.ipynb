{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd941a8",
   "metadata": {},
   "source": [
    "# PageRank Algorithmus\n",
    "\n",
    "PageRank ist ein Algorithmus um zu entscheiden wie relevant Websites sind im Verhältins zueinander. Websites werden modelliert wie Knoten oder Ecken in einem gerichten Graph und die Kanten kommen von Links zwischen Websites. Nehme zum Beispiel ein Internet mit nur vier Websites: $w_0$, $w_1$, $w_2$ und $w_3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = DiGraph({0:[1,2,3],1:[2],2:[3,1]})\n",
    "D.show(layout='circular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1d981",
   "metadata": {},
   "source": [
    "Im obigen Figur enthält $w_2$ also wenigstens zwei Links, wenigstens einen davon geht nach $w_1$ und wenigstens eienen nach $w_3$, alle übrigen gehen entwered nach $w_1$ oder $w_3$. Das heißt, auch wenn es mehere Links von $w_2$ nach $w_1$ gäbe, würden die alle mit nur einer Kante bezeignet worden. Jetzt postulieren wir dass, wenn wir das Internet benutzen, wir genau an einem Website sind. Die Relevanz einer Website ist dann die Change das wir uns an dieser Website befinden. Präziser definieren wir die Menge der stochastischen Bewertungen der Websites im Internet $G = (V, E, s, t)$ wie\n",
    "\n",
    "$$\n",
    "PR(G) = \\left\\{ (r_v)_{v \\in V} \\in \\mathbb{R}^V \\mid \\sum_{v \\in V} r_v = 1 \\right\\}.\n",
    "$$\n",
    "\n",
    "In unserem Beispiel ist also $V = \\{ w_0, w_1, w_2, w_3 \\}$ und $E$ ist die Menge der sechs Kanten die von $s,t: E \\rightarrow V$ Anfang und Ende zugewiesen werden.\n",
    "\n",
    "Weiter nehmen wir an, dass ein Benutzer die Links auf einer Website benutzt um zu navigieren und dass jede gelinktete Website mit demsleben Wahrscheinlichkeit erreicht wird. Wenn er sich an einer Website $w_i$ befindet, ist die Wahrscheinlichkeit dass er sich als nächtes an der Website $w_j$ befindet also:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(w_j \\text{wird am nächsten besucht } \\mid w_j \\text{ wird jetzt besucht}) = \\begin{cases}\n",
    "\\frac{1}{k} &, \\exists e \\in E, s(e) = w_i \\wedge t(e) = w_j\\\\\n",
    "0 & \\text{sonst}\n",
    "\\end{cases}\n",
    "\\newline\n",
    "k = \\left| \\left\\{ e \\in E \\mid s(e) = w_i \\right\\} \\right|\n",
    "$$\n",
    "\n",
    "Diese Situation können wir mit einer Matrix $A$ beschreiben. Setzen wir\n",
    "\n",
    "$$\n",
    "A_{ji} = \\mathbb{P}(w_j \\text{wird am nächsten besucht } \\mid w_j \\text{ wird jetzt besucht}),\n",
    "$$\n",
    "\n",
    "dann lasst unseres Beispiel sich durch der folgenden Matrix beschreiben\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 0 & 0\\\\\n",
    "\\frac{1}{3} & 0 & \\frac{1}{2} & 0\\\\\n",
    "\\frac{1}{3} & 1 & 0 & 0\\\\\n",
    "\\frac{1}{3} & 0 & \\frac{1}{2} & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Es gibt verschiedene Probleme die auftreten können, wir sehen nähmlich schön, dass unter unsere annahmen, die Website $w_3$ nie verlassen werden kann. Dass ist nicht sehr realistisch. Eine lösung ist um zu jedem Link nach $w_3$, einen Link in der Rückrichtung hinzu zu fügen. Dies würde meinen dass wir den 'vorherige'-knopf im Browser benutzen. Aber meistens werden in so einer Situation keine Annahmen gemacht über welche Website der Benutzer am nächsten besucht, d.h. $A_{j3} = \\frac{1}{|G|}$ $j = 0, \\dots ,3$. Die Matrix wird dann so aussehen:\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 0 & \\frac{1}{4}\\\\\n",
    "\\frac{1}{3} & 0 & \\frac{1}{2} & \\frac{1}{4}\\\\\n",
    "\\frac{1}{3} & 1 & 0 & \\frac{1}{4}\\\\\n",
    "\\frac{1}{3} & 0 & \\frac{1}{2} & \\frac{1}{4}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Ein zweites Problem kann sein, dass der Graph $G$ nicht zusammenhängend ist. Es kann also mehrere Zusammenhangskomonenten geben. Obwohl diese Situation nicht wahrscheinlich ist, weil das Internet einfach zu groß ist, ist sie in unsere kleine Beispiele sicher nicht undenkbar. Eine Lösung zu diesem Problem ist eine Buffermatrix zu nehmen\n",
    "\n",
    "$$\n",
    "B = \\frac{1}{|G|}\\begin{pmatrix}\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "1 & \\ddots & & 1 \\\\\n",
    "\\vdots & &  & \\vdots\\\\\n",
    "1 & 1 & \\cdots & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\qquad M = (1 - p)A + pB \\qquad p \\in [0,1]\n",
    "$$\n",
    "\n",
    "Die Zahl $p$ heißt damping factor und stellt die Wahrscheinlichkeit dar, dass ein benutzer nicht mit den Links auf den Websites navigiert, sonder random eine neue Adresse eintyp. M ist dann eine Matrix mit schöne eigenschaften\n",
    "\n",
    "* $M_{ij} > 0 \\quad \\forall i,j$, mann sagt $M$ ist positiv\n",
    "* $\\sum_{i = 1}^{|G|} M_{ij} = 1 \\quad \\forall j$, mann sagt $M$ ist spaltenstochastisch \n",
    "\n",
    "**Perron-Frobenius Satz**\n",
    "\n",
    "Es sei $M$ eine positive Matrix, dann existiers es genau einen größten reellen Eigenwert $\\rho$ von geometrische Vielfachkeit $1$ und alle sonstige Eigenwerte sind kleiner als $1$. Außerdem ist ein Eigenvektor von $\\rho$ entweder positiv oder negativ.\n",
    "\n",
    "**Korrolar**\n",
    "\n",
    "Ist $M$ spaltenstochatisch, dann ist $\\rho = 1$\n",
    "\n",
    "**Aufgabe**\n",
    "\n",
    "Zeigen Sie, dass die eigenschaft spaltenstochastisch zu sein, unter Multiplikation erhalten bleibt, d.h\n",
    "\n",
    "$$\n",
    "A, B \\text{ sind spaltenstochastisch, dann ist auch } AB \\text{ spaltenstochastisch.}\n",
    "$$\n",
    "\n",
    "Schauen wir uns jetzt den Eigenvektor $v$ von $1$ an, die positiv ist und spaltenstochastisch, d.h.\n",
    "\n",
    "$$\\sum_{i = 1}^{|G|} v_i = 1,$$\n",
    "\n",
    "dann können wir $v_i$ sehen wie die Wahrscheinlichkeit dass ein Benutzer des Internets sich an der Website $w_i$ befindet. Weil $Mv  =v$ gilt ist diese Auslegung auch noch sinnvoll wenn einmal, und deswegen endlich oft, navigiert wird.\n",
    "\n",
    "Aber was passiert wenn wir voraus setzen, dass jede Website am Anfang die gleiche Wahrscheinlichkeit hat besucht zu werden?\n",
    "\n",
    "$$\n",
    "v_0 = \\frac{1}{|G|}\\begin{pmatrix}1\\\\1\\\\ \\vdots \\\\ 1\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Dann ist die Wahrscheinlichkeitsverteilung der aktuelle Position in $G$ nach $n$ Mal navigieren, gegeben durch\n",
    "\n",
    "$$A^n v_0$$\n",
    "\n",
    "und ganz naiv machen wir die folgende Bemerkung\n",
    "\n",
    "$$\n",
    "\\text{lim}_{n\\rightarrow \\infty} A^n v_0 = \\text{lim}_{n\\rightarrow \\infty} A A^{n-1} v_0 = A \\text{lim}_{n\\rightarrow \\infty} A^n v_0\n",
    "$$\n",
    "\n",
    "Dies würde implizieren, dass\n",
    "\n",
    "$$\n",
    "\\text{lim}_{n\\rightarrow \\infty} A^n v_0 = \\lambda v \\qquad \\lambda \\in \\mathbb{R}^*\n",
    "$$\n",
    "\n",
    "Das ist auch so weil $A$ ein Eigenwert $1$ hat, so dass $A^n$ nicht gegen Null konvergiert. Weiter sind die übrige Eigenwerte kleiner als $1$, so dass $A^n$ nicht divergiert. Weil $v_0$ positiv ist, ist $\\lambda$ das auch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9670c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = DiGraph({0:[1,2,3,7,8],1:[2,5,6,9],2:[1,3,4,6,8],3:[2,5,9],4:[],5:[1,3,4,6,8,9],6:[1,2,4,8],7:[3,5,9],8:[5,7],9:[1,5,8]})\n",
    "G.show(layout='circular')\n",
    "\n",
    "\n",
    "def transitionmatrix(graph, dampingfactor):\n",
    "    g = len(graph.vertices())\n",
    "    tm = matrix(QQ, g, g, lambda i, j: 1 / len(graph.neighbors_out(j)) if (i in graph.neighbors_out(j)) else 0)\n",
    "    ones = matrix(QQ, 1, g, lambda i, j: 1)\n",
    "    for i in range(g):\n",
    "        if ones*tm[:,i] == 0:\n",
    "            tm[:,i] = 1/g * transpose(ones)\n",
    "    if dampingfactor > 0:\n",
    "        dampingmatrix = matrix(QQ, g, g, lambda i, j: 1 / g)\n",
    "        tm = (1 - dampingfactor)*tm + dampingfactor*dampingmatrix\n",
    "    return tm\n",
    "\n",
    "A = transitionmatrix(G,0)\n",
    "\n",
    "# A.eigenvectors_right() gibt Tripel (l, ev, m) mit\n",
    "# l ein Eigenwert\n",
    "# ev eine Liste mit zugehörigen Eigenvektoren\n",
    "# m die zugehörige Vielfachkeit\n",
    "v = transpose(matrix(A.eigenvectors_right()[0][1][0]))\n",
    "\n",
    "print(A)\n",
    "print(norm(v - A*v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2434a",
   "metadata": {},
   "source": [
    "Es ist auch sehr interessant darüber nach zu denken, was ein damping factor genau mit der Auskunft des Verfahrens tut. Was sagt Ihre Intuition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077c60a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(v.n())\n",
    "print()\n",
    "print(transpose(matrix(transitionmatrix(G,1/10).eigenvectors_right()[0][1][0])).n())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49a58d",
   "metadata": {},
   "source": [
    "Jetzt werden wir versuchen zu zeigen, dass $A^n v_0$ gegen $v$ konvergiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "if v[0] < 0:\n",
    "    v = -1*v\n",
    "v = (1 / (ones*v)[0][0]) * v\n",
    "v0 = transpose((1/g)* ones)\n",
    "\n",
    "# ändern Sie n >> 0 und begeistern Sie sich für das Ergebnis\n",
    "n = 1\n",
    "print(norm(A^n * v0 - v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc39fcd",
   "metadata": {},
   "source": [
    "**Aufgabe**\n",
    "\n",
    "Berechenen Sie den Eigenvektor zugehörig zum Eigenwert $1$ in der folgenden Situation, mit $p = 0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9898eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gsmall = DiGraph({0:[2], 1:[2]})\n",
    "Gsmall.show(layout='circular')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28d04d",
   "metadata": {},
   "source": [
    "Kontrollieren Sie unten Ihre Antwort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Asmall = transitionmatrix(Gsmall,0)\n",
    "\n",
    "vsmall = Asmall.right_eigenvectors()[0][1][0]/5\n",
    "print(vsmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fe2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "v0small = vector([1/3,1/3,1/3])\n",
    "zero = vector([0,0,0])\n",
    "\n",
    "for i in range(14):\n",
    "    clear_output(wait=true)\n",
    "    S = (arrow(zero,vsmall) + arrow(zero,Asmall^i * v0small,color=(1,0,0)) + arrow((0,0,0),(1,0,0),width=0) +arrow((0,0,0),(0,1,0),width=0) + arrow((0,0,0),(0,0,1),width=0))\n",
    "    S.rotate([0,1,-1],pi/2).show(aspect_ratio=[1,1,1],frame_aspect_ratio=1,axes=True)\n",
    "    sleep(float(0.75))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.8",
   "language": "sage",
   "name": "sagemath-9.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
